# Caresma System Flow Diagram

## Complete Audio-to-Avatar Flow

This document describes the complete flow from when a user speaks to when the AI avatar responds.

---

## ğŸ¯ Key Features

### âœ… Automatic Voice Activity Detection (VAD)
The system uses **OpenAI's built-in VAD** to automatically detect when users stop speaking:
- **No button click required** - Just speak naturally and pause
- **500ms silence threshold** - Responds automatically after half-second pause
- **Configurable sensitivity** - Adjust detection threshold and silence duration
- **Manual override available** - "Stop Recording" button for immediate response

### ğŸ”„ Dual Detection Modes
1. **Automatic (Default)** â­
   - OpenAI VAD monitors audio in real-time
   - Auto-commits after 500ms of silence
   - Natural conversation flow

2. **Manual (Override)**
   - User clicks "Stop Recording" button
   - Immediate response without waiting
   - User control when needed

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              FRONTEND (React)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  useOpenAIWebSocketâ”‚          â”‚  useHeygenAvatar     â”‚               â”‚
â”‚  â”‚  Hook              â”‚          â”‚  Hook                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â”‚                                 â”‚                             â”‚
â”‚           â”‚ WebSocket                       â”‚ WebRTC                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â”‚                                 â”‚
            â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BACKEND (FastAPI)   â”‚         â”‚   HeyGen Service     â”‚
â”‚   WebSocket Handler   â”‚         â”‚   (Cloud)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI Realtime API â”‚
â”‚   (ASR + LLM)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Flow: User Speaks â†’ Avatar Responds

### Phase 1: Session Initialization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚                                  â”‚ Backend  â”‚                 â”‚ HeyGen  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                                            â”‚                            â”‚
     â”‚ 1. POST /heygen/session-token             â”‚                            â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                            â”‚
     â”‚                                            â”‚                            â”‚
     â”‚                                            â”‚ 2. Create token            â”‚
     â”‚                                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                                            â”‚                            â”‚
     â”‚                                            â”‚ 3. Return token            â”‚
     â”‚                                            â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                                            â”‚                            â”‚
     â”‚ 4. { token: "jwt..." }                    â”‚                            â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                            â”‚
     â”‚                                            â”‚                            â”‚
     â”‚ 5. new StreamingAvatar({ token })          â”‚                            â”‚
     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                                            â”‚                            â”‚
     â”‚                                            â”‚                            â”‚
     â”‚ 6. WebRTC connection established           â”‚                            â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚    (video stream starts)                   â”‚                            â”‚
     â”‚                                            â”‚                            â”‚
     â”‚ 7. Connect WebSocket                       â”‚                            â”‚
     â”‚    ws://backend/ws/session/{id}            â”‚                            â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                            â”‚
     â”‚                                            â”‚                            â”‚
     â”‚ 8. WebSocket connected                     â”‚                            â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                            â”‚
     â”‚                                            â”‚                            â”‚
```

### Phase 2: User Speaks - Audio Capture & Streaming

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚                    â”‚ Backend  â”‚                    â”‚ OpenAI   â”‚
â”‚         â”‚                    â”‚ WebSocketâ”‚                    â”‚ Realtime â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚                               â”‚
     â”‚ 1. User clicks                â”‚                               â”‚
     â”‚    "Start Microphone"         â”‚                               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 2. navigator.mediaDevices     â”‚                               â”‚
     â”‚    .getUserMedia()            â”‚                               â”‚
     â”‚    âœ“ Get microphone access    â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 3. Send JSON message          â”‚                               â”‚
     â”‚    { type: "start_recording" }â”‚                               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 4. Backend marks              â”‚
     â”‚                               â”‚    recording active           â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 5. Response                   â”‚                               â”‚
     â”‚    { type: "recording_started"}â”‚                               â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 6. USER SPEAKS INTO MIC       â”‚                               â”‚
     â”‚    ğŸ¤ Audio capture begins    â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 7. ScriptProcessor captures   â”‚                               â”‚
     â”‚    audio in 4096 sample chunksâ”‚                               â”‚
     â”‚    @ 24kHz sample rate        â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 8. Convert Float32 â†’ PCM16    â”‚                               â”‚
     â”‚    (16-bit signed integers)   â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 9. Send binary PCM16 data     â”‚                               â”‚
     â”‚    <audio bytes> (every ~170ms)â”‚                              â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 10. Accumulate audio          â”‚
     â”‚                               â”‚     in buffer                 â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 11. Forward to OpenAI         â”‚
     â”‚                               â”‚     Realtime API              â”‚
     â”‚                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚ 12. OpenAI streams
     â”‚                               â”‚                               â”‚     partial transcripts
     â”‚                               â”‚                               â”‚     (ASR in progress)
     â”‚                               â”‚                               â”‚
```

### Phase 3: Stop Speaking Detection & Processing

**IMPORTANT: There are TWO ways the system detects when the user stops speaking:**

#### Option A: Automatic Voice Activity Detection (VAD) - DEFAULT BEHAVIOR â­

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚                    â”‚ Backend  â”‚                    â”‚ OpenAI   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚                               â”‚
     â”‚ 1. User is speaking           â”‚                               â”‚
     â”‚    Audio streaming continues  â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚ 2. OpenAI VAD monitors
     â”‚                               â”‚                               â”‚    audio in real-time
     â”‚                               â”‚                               â”‚
     â”‚ 3. User stops speaking        â”‚                               â”‚
     â”‚    (pauses/goes silent)       â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚ 4. VAD detects silence
     â”‚                               â”‚                               â”‚    for 500ms
     â”‚                               â”‚                               â”‚    â±ï¸  Threshold met!
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚ 5. OpenAI AUTO-COMMITS
     â”‚                               â”‚                               â”‚    audio buffer
     â”‚                               â”‚                               â”‚    (no manual trigger!)
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚ 6. OpenAI processes:
     â”‚                               â”‚                               â”‚    - Finalize ASR
     â”‚                               â”‚                               â”‚    - Run LLM
     â”‚                               â”‚                               â”‚    - Generate response
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 7. Final transcript           â”‚
     â”‚                               â”‚    "What's the weather?"      â”‚
     â”‚                               â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                               â”‚                               â”‚
     â”‚ 8. Send transcript to UI      â”‚                               â”‚
     â”‚    { type: "transcript",      â”‚                               â”‚
     â”‚      text: "What's..." }      â”‚                               â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 9. UPDATE UI:                 â”‚                               â”‚
     â”‚    setUserTranscript(text)    â”‚                               â”‚
     â”‚    âœ… "You: What's the        â”‚                               â”‚
     â”‚        weather?"              â”‚                               â”‚
     â”‚                               â”‚                               â”‚
```

**VAD Configuration (in backend):**
```python
# File: app/services/openai_service.py:307-312
"turn_detection": {
    "type": "server_vad",           # Server-side Voice Activity Detection
    "threshold": 0.5,                # Sensitivity (0.0-1.0)
    "prefix_padding_ms": 300,        # Include 300ms before speech
    "silence_duration_ms": 500       # Auto-commit after 500ms silence
}
```

#### Option B: Manual Stop Recording Button - OPTIONAL OVERRIDE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚                    â”‚ Backend  â”‚                    â”‚ OpenAI   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚                               â”‚
     â”‚ 1. User clicks                â”‚                               â”‚
     â”‚    "Stop Recording" button    â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 2. stopRecording()            â”‚                               â”‚
     â”‚    - Disconnect audio processorâ”‚                              â”‚
     â”‚    - Stop media tracks        â”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 3. Send JSON message          â”‚                               â”‚
     â”‚    { type: "stop_recording" } â”‚                               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 4. Backend detects            â”‚
     â”‚                               â”‚    "stop_recording"           â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 5. Manual commit signal       â”‚
     â”‚                               â”‚    to OpenAI (finalize)       â”‚
     â”‚                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                               â”‚    { type: "input_audio_      â”‚
     â”‚                               â”‚      buffer.commit" }         â”‚
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚                               â”‚ 6. OpenAI processes:
     â”‚                               â”‚                               â”‚    - Finalize ASR
     â”‚                               â”‚                               â”‚    - Run LLM
     â”‚                               â”‚                               â”‚    - Generate response
     â”‚                               â”‚                               â”‚
     â”‚                               â”‚ 7. Final transcript           â”‚
     â”‚                               â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                               â”‚                               â”‚
     â”‚ 8. Send transcript to UI      â”‚                               â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚
     â”‚                               â”‚                               â”‚
     â”‚ 9. UPDATE UI                  â”‚                               â”‚
     â”‚    âœ… Display transcript      â”‚                               â”‚
     â”‚                               â”‚                               â”‚
```

**Code Locations:**
- Frontend click handler: [App.js:208-209](../src/App.js#L208-L209)
- Frontend stopRecording(): [useOpenAIWebSocket.js:157-180](../src/hooks/useOpenAIWebSocket.js#L157-L180)
- Backend handler: [websocket.py:117-121](../../caresma-backend/app/api/v1/websocket.py#L117-L121)
- Backend commit: [openai_service.py:123-147](../../caresma-backend/app/services/openai_service.py#L123-L147)

---

**KEY POINTS:**

1. **Default Behavior = Automatic VAD** â­
   - User speaks â†’ pauses for 500ms â†’ OpenAI auto-detects â†’ response generated
   - No button click required!
   - More natural conversation flow

2. **Manual Button = Override**
   - User can click "Stop Recording" to force immediate response
   - Useful when user wants to respond before 500ms silence threshold
   - Bypasses VAD wait time

3. **When is transcript populated?**
   - Backend sends `{ type: "transcript", text: "..." }` after OpenAI processes speech
   - Frontend receives it at [useOpenAIWebSocket.js:46-48](../src/hooks/useOpenAIWebSocket.js#L46-L48)
   - Calls `setUserTranscript(text)` to update UI
   - Works the same for both VAD and manual stop

### Phase 4: AI Response Generation & Display

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚                    â”‚ Backend  â”‚                    â”‚ OpenAI   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚                               â”‚
     â”‚                              â”‚ 1. OpenAI LLM generates       â”‚
     â”‚                              â”‚    response text              â”‚
     â”‚                              â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                              â”‚    "It's sunny and 72Â°F"      â”‚
     â”‚                              â”‚                               â”‚
     â”‚ 2. Send AI response          â”‚                               â”‚
     â”‚    { type: "text_response",  â”‚                               â”‚
     â”‚      text: "It's sunny..." } â”‚                               â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚
     â”‚                              â”‚                               â”‚
     â”‚ 3. UPDATE UI:                â”‚                               â”‚
     â”‚    setAiResponse(text)       â”‚                               â”‚
     â”‚    Display in conversation   â”‚                               â”‚
     â”‚    âœ… "Assistant: It's       â”‚                               â”‚
     â”‚        sunny and 72Â°F"       â”‚                               â”‚
     â”‚                              â”‚                               â”‚
```

**KEY POINT - When is AI response populated?**
- AI response appears in the UI when backend sends `{ type: "text_response" }`
- Frontend receives it and calls `setAiResponse(text)`
- This happens at line 42-43 in useOpenAIWebSocket.js

### Phase 5: Avatar Speaks - Text to Video

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚                                              â”‚ HeyGen   â”‚
â”‚         â”‚                                              â”‚ Service  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                                                        â”‚
     â”‚ 1. Receive text_response                              â”‚
     â”‚    (from Phase 4)                                     â”‚
     â”‚                                                        â”‚
     â”‚ 2. onTextResponseRef callback                         â”‚
     â”‚    triggered in App.js (line 39-44)                   â”‚
     â”‚                                                        â”‚
     â”‚ 3. Call speak(text)                                   â”‚
     â”‚    from useHeygenAvatar hook                          â”‚
     â”‚                                                        â”‚
     â”‚ 4. avatarRef.current.speak({                          â”‚
     â”‚      text: "It's sunny and 72Â°F",                     â”‚
     â”‚      taskType: "repeat"                               â”‚
     â”‚    })                                                 â”‚
     â”‚                                                        â”‚
     â”‚ 5. HeyGen SDK sends text                              â”‚
     â”‚    via WebRTC data channel                            â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                                                        â”‚
     â”‚                                                        â”‚ 6. HeyGen:
     â”‚                                                        â”‚    - Text-to-Speech
     â”‚                                                        â”‚    - Lip sync generation
     â”‚                                                        â”‚    - Avatar video render
     â”‚                                                        â”‚
     â”‚ 7. Stream video frames back                           â”‚
     â”‚    via WebRTC (H.264)                                 â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                                                        â”‚
     â”‚ 8. Video element displays                             â”‚
     â”‚    avatar speaking the text                           â”‚
     â”‚    ğŸ¬ Avatar lips move!                               â”‚
     â”‚                                                        â”‚
     â”‚ 9. Event: AVATAR_START_TALKING                        â”‚
     â”‚    setIsSpeaking(true)                                â”‚
     â”‚    âœ… Status: "Avatar is speaking..."                 â”‚
     â”‚                                                        â”‚
     â”‚ 10. Event: AVATAR_STOP_TALKING                        â”‚
     â”‚     setIsSpeaking(false)                              â”‚
     â”‚     âœ… Status: "Ready"                                â”‚
     â”‚                                                        â”‚
```

---

## How Backend Detects "User Stopped Speaking"

**âœ… AUTOMATIC DETECTION IS ENABLED** - The system uses OpenAI's built-in Voice Activity Detection (VAD).

### Method 1: Automatic VAD (Default - ENABLED â­)

```javascript
User speaks â†’ pauses for 500ms
  â†“
OpenAI VAD automatically detects silence
  â†“
OpenAI auto-commits audio buffer (no manual trigger!)
  â†“
OpenAI processes â†’ sends transcript & response to backend
  â†“
Backend forwards to frontend â†’ UI updates
```

**Configuration:** [openai_service.py:307-312](../../caresma-backend/app/services/openai_service.py#L307-L312)
```python
"turn_detection": {
    "type": "server_vad",           # âœ… Server-side VAD enabled
    "threshold": 0.5,                # Sensitivity (0.0-1.0)
    "prefix_padding_ms": 300,        # Include 300ms before speech
    "silence_duration_ms": 500       # â±ï¸  Auto-commit after 500ms silence
}
```

**How it works:**
1. OpenAI monitors audio stream in real-time
2. Detects when user stops speaking (500ms silence)
3. Automatically commits audio buffer
4. Processes ASR + LLM
5. Sends results back to backend

### Method 2: Manual Control (Optional Override)

```javascript
// User can manually force response by clicking button
stopRecording()  // User clicks "Stop Recording" button
  â†“
ws.send({ type: "stop_recording" })
  â†“
Backend receives signal â†’ commits audio â†’ OpenAI processes
```

**When to use manual control:**
- User wants immediate response (don't wait for 500ms)
- User finished speaking but hasn't paused long enough
- Override VAD automatic detection

---

**Current System:** Uses **both** methods:
- **Primary:** Automatic VAD detection (500ms silence threshold)
- **Backup:** Manual "Stop Recording" button for user override
- This provides the best of both worlds: natural conversation + user control

---

## Timeline of State Changes

### User Transcript Population

#### With Automatic VAD (Default):

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                                                   time

User starts speaking       User stops speaking      Transcript appears in UI
        â”‚                   (500ms silence)                   â”‚
        â”‚   Recording active    â”‚   VAD auto-detects         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
        â”‚   Audio streaming     â”‚   OpenAI processes         â”‚
        â”‚                       â”‚                            â”‚
        â–¼                       â–¼                            â–¼
   isRecording=true      OpenAI VAD triggers       userTranscript = "..."
                         (automatic)                âœ… Displayed in UI
```

**Key Point:** With VAD, there's a delay of ~0.5-2 seconds:
- User pauses for 500ms
- OpenAI VAD auto-detects silence
- OpenAI processes audio
- Transcript appears in frontend UI

#### With Manual Button:

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                                                   time

User starts speaking          User clicks stop           Transcript appears in UI
        â”‚                            â”‚                            â”‚
        â”‚   Recording active         â”‚   Manual commit           â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
        â”‚   Audio streaming          â”‚   OpenAI processes        â”‚
        â”‚                            â”‚                            â”‚
        â–¼                            â–¼                            â–¼
   isRecording=true           isRecording=false          userTranscript = "..."
                                                         âœ… Displayed in UI
```

**Key Point:** Manual control bypasses 500ms wait but adds UI interaction time

### AI Response Population

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                                                   time

Transcript received        OpenAI LLM thinks           AI response appears
        â”‚                         â”‚                            â”‚
        â”‚   Backend processes     â”‚   Response generated       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
        â”‚   GPT-4 generation      â”‚                            â”‚
        â”‚                         â”‚                            â”‚
        â–¼                         â–¼                            â–¼
   status: "Processing..."   Generating response...    aiResponse = "..."
                                                        âœ… Displayed in UI
```

**Key Point:** AI response typically takes 1-5 seconds to generate, depending on:
- Length of user input
- Complexity of response
- OpenAI API latency

### Avatar Speaking

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                                                   time

AI response received       HeyGen processes           Avatar speaks
        â”‚                         â”‚                         â”‚
        â”‚   speak(text) called    â”‚   TTS + lip sync       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
        â”‚   HeyGen API call       â”‚   Video generation      â”‚
        â”‚                         â”‚                         â”‚
        â–¼                         â–¼                         â–¼
   Send text to HeyGen       Processing...         isSpeaking = true
                                                    ğŸ¬ Video plays

                                                    Avatar finishes
                                                          â”‚
                                                          â–¼
                                                    isSpeaking = false
```

**Key Point:** There's a delay of ~500ms-2s between:
- Frontend calls `speak(text)`
- Avatar starts visibly speaking (lips moving)

---

## Data Formats

### Audio Format (Frontend â†’ Backend)
```
Format: PCM16 (signed 16-bit integers)
Sample Rate: 24,000 Hz (24 kHz)
Channels: 1 (mono)
Chunk Size: 4096 samples (~170ms of audio)
Byte Size: 8192 bytes per chunk (4096 samples Ã— 2 bytes)
Transport: Binary WebSocket frames
```

### WebSocket Messages

#### Frontend â†’ Backend
```typescript
// JSON Messages
{ type: "start_recording" }
{ type: "stop_recording" }
{ type: "ping" }

// Binary Messages
<PCM16 audio bytes>
```

#### Backend â†’ Frontend
```typescript
// Status updates
{ type: "recording_started" }
{ type: "recording_stopped" }
{ type: "pong" }

// Transcription
{
  type: "transcript",
  text: "What's the weather like?"
}

// AI Response
{
  type: "text_response",
  text: "It's sunny and 72 degrees."
}

// Errors
{
  type: "error",
  message: "Connection failed"
}
```

---

## Key Components & Responsibilities

### Frontend

#### `useOpenAIWebSocket` Hook
**Responsibilities:**
- Establish WebSocket connection to backend
- Capture microphone audio
- Convert audio to PCM16 format
- Stream audio chunks to backend
- Receive transcripts and AI responses
- Manage recording state

**Key Functions:**
- `startRecording()` - Start audio capture
- `stopRecording()` - Stop audio capture
- `setOnTextResponse(callback)` - Register AI response handler
- `setOnTranscript(callback)` - Register transcript handler

#### `useHeygenAvatar` Hook
**Responsibilities:**
- Get session token from backend
- Initialize HeyGen Streaming Avatar SDK
- Establish WebRTC connection to HeyGen
- Receive avatar video stream
- Control avatar speech
- Track avatar speaking state

**Key Functions:**
- `speak(text)` - Make avatar speak text
- `stopSpeaking()` - Interrupt avatar
- `closeAvatar()` - End avatar session

#### `App.js`
**Responsibilities:**
- Coordinate between audio and avatar
- Display conversation UI
- Handle user interactions
- Manage session lifecycle

**Key State:**
- `sessionStarted` - Whether session is active
- `userTranscript` - User's speech (populated after stop recording)
- `aiResponse` - AI's text response (populated when received)
- `isRecording` - Whether actively recording audio
- `isSpeaking` - Whether avatar is speaking

### Backend

#### WebSocket Handler (`/ws/session/{session_id}`)
**Responsibilities:**
- Accept WebSocket connections
- Receive audio chunks from frontend
- Buffer audio data
- Connect to OpenAI Realtime API
- Forward audio to OpenAI
- Receive transcripts and responses from OpenAI
- Send transcripts and responses to frontend

**Key Events Handled:**
- `start_recording` - Start audio session
- `stop_recording` - Finalize audio, trigger processing
- Binary frames - Audio data

#### HeyGen Token Service (`/heygen/session-token`)
**Responsibilities:**
- Create HeyGen session tokens
- Secure token generation
- Return token to frontend

---

## Error Handling

### Connection Errors
```
Frontend detects:
- WebSocket disconnect
- Avatar stream disconnect
- Microphone permission denied

Actions:
- Display error message
- Allow retry
- Clean up resources
```

### Audio Errors
```
Backend detects:
- OpenAI API errors
- Invalid audio format
- Timeout

Actions:
- Send error message to frontend
- Log error details
- Reset session state
```

---

## Performance Considerations

### Latency Breakdown
```
User stops speaking
  â†“ ~100ms - Network transit
Backend receives stop signal
  â†“ ~500ms - OpenAI ASR finalization
Transcript generated
  â†“ ~50ms - Network transit
Frontend displays transcript
  â†“ ~2000ms - OpenAI LLM generation
AI response generated
  â†“ ~50ms - Network transit
Frontend receives response
  â†“ ~100ms - speak() call
HeyGen receives text
  â†“ ~1000ms - TTS + lip sync
Avatar starts speaking
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~3.8 seconds (typical)
```

### Optimization Opportunities
1. **Reduce OpenAI latency** - Use streaming responses
2. **Pre-warm HeyGen** - Keep session alive
3. **Use VAD** - Auto-detect speech end
4. **Optimize audio chunks** - Smaller chunks = faster processing
5. **Parallel processing** - Start LLM while finalizing ASR

---

## Summary

### Complete Flow in Simple Terms:

1. **User clicks "Start Session"**
   - Frontend gets HeyGen token from backend
   - Frontend connects to HeyGen (WebRTC video starts)
   - Frontend connects to backend (WebSocket opens)

2. **User clicks "Start Microphone"**
   - Browser asks for microphone permission
   - Audio capture begins (24kHz, PCM16)
   - Audio streams to backend every ~170ms

3. **User speaks**
   - Audio continuously streams
   - Backend forwards to OpenAI
   - OpenAI VAD monitors for silence in real-time

4. **User stops speaking (pauses for 500ms) OR clicks "Stop Recording"**
   - **Automatic (VAD):** OpenAI detects 500ms silence and auto-commits
   - **Manual:** User clicks button, backend sends commit signal
   - OpenAI generates final transcript
   - **Transcript appears in UI** â† This is when userTranscript is populated

5. **OpenAI generates response**
   - Backend sends to frontend
   - **AI response appears in UI** â† This is when aiResponse is populated
   - Frontend calls `speak(text)` on HeyGen avatar

6. **Avatar speaks**
   - HeyGen converts text to speech + video
   - Video streams back via WebRTC
   - Avatar lips move, user sees response

---

## FAQ

**Q: When exactly does the user transcript appear?**
A: After the user stops speaking for 500ms (VAD auto-detects) OR clicks "Stop Recording", OpenAI processes the audio and sends the final transcript back. This takes ~0.5-2 seconds with VAD, ~1-3 seconds with manual stop.

**Q: How does the backend know the user stopped speaking?**
A: **Two ways:**
1. **Automatic (Default):** OpenAI Realtime API has built-in Voice Activity Detection (VAD) that monitors audio and auto-detects 500ms of silence
2. **Manual (Override):** User clicks "Stop Recording" button, frontend sends `{ type: "stop_recording" }` to backend

**Q: Can we adjust the VAD sensitivity?**
A: Yes! Edit the configuration in [openai_service.py:307-312](../../caresma-backend/app/services/openai_service.py#L307-L312):
```python
"turn_detection": {
    "type": "server_vad",
    "threshold": 0.5,              # â† Adjust 0.0-1.0 (lower = more sensitive)
    "silence_duration_ms": 500     # â† Change milliseconds (300-1500 recommended)
}
```

**Q: Why is there a delay before the avatar speaks?**
A: Multiple steps: OpenAI ASR (500ms) â†’ OpenAI LLM (2s) â†’ Network (50ms) â†’ HeyGen TTS+lipsync (1s) = ~3.5 seconds total.

**Q: Does the video come from the backend?**
A: No! Video comes directly from HeyGen to the browser via WebRTC. Backend only handles audio/text. This keeps video quality high and latency low.

---

## Architecture Diagram (ASCII)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE (Browser)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Video       â”‚  â”‚ Conversation â”‚  â”‚  Controls    â”‚                  â”‚
â”‚  â”‚  (Avatar)    â”‚  â”‚  Display     â”‚  â”‚  (Buttons)   â”‚                  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚                  â”‚
â”‚  â”‚  - Shows     â”‚  â”‚ - User text  â”‚  â”‚ - Start Mic  â”‚                  â”‚
â”‚  â”‚    HeyGen    â”‚  â”‚ - AI text    â”‚  â”‚ - Stop Mic   â”‚                  â”‚
â”‚  â”‚    avatar    â”‚  â”‚              â”‚  â”‚ - End Sessionâ”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                 â”‚                  â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                  â”‚
          â”‚ WebRTC          â”‚ State Updates    â”‚ Events
          â”‚ (video)         â”‚ (text)           â”‚ (clicks)
          â”‚                 â”‚                  â”‚
          â”‚                 â–¼                  â”‚
          â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
          â”‚         â”‚   App.js       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚         â”‚   (Coordinator)â”‚
          â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚
          â”‚                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                 â–¼                  â–¼                  â–¼
          â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         â”‚useHeygenAvatar â”‚  â”‚useOpenAI     â”‚  â”‚   State      â”‚
          â”‚         â”‚                â”‚  â”‚WebSocket     â”‚  â”‚   Variables  â”‚
          â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
                    â”‚                          â”‚
                    â”‚ WebRTC                   â”‚ WebSocket
                    â”‚                          â”‚
                    â–¼                          â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  HeyGen Cloud   â”‚        â”‚  Backend        â”‚
          â”‚  Service        â”‚        â”‚  (FastAPI)      â”‚
          â”‚                 â”‚        â”‚                 â”‚
          â”‚  - TTS          â”‚        â”‚  - Audio buffer â”‚
          â”‚  - Lip sync     â”‚        â”‚  - WebSocket    â”‚
          â”‚  - Video render â”‚        â”‚    handler      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â”‚ HTTP API
                                               â”‚
                                               â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚  OpenAI         â”‚
                                     â”‚  Realtime API   â”‚
                                     â”‚                 â”‚
                                     â”‚  - ASR (Speech  â”‚
                                     â”‚    to Text)     â”‚
                                     â”‚  - LLM (GPT-4)  â”‚
                                     â”‚  - VAD (Voice   â”‚
                                     â”‚    Activity     â”‚
                                     â”‚    Detection)   â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Reference: VAD Configuration

### Current Settings (Production)
```python
# File: app/services/openai_service.py:307-312
"turn_detection": {
    "type": "server_vad",
    "threshold": 0.5,
    "prefix_padding_ms": 300,
    "silence_duration_ms": 500
}
```

### Recommended Settings by Use Case

| Use Case | `silence_duration_ms` | `threshold` | Description |
|----------|----------------------|-------------|-------------|
| **Fast Chat** | 300-400ms | 0.4-0.5 | Quick responses, may cut off slower speakers |
| **Natural Conversation** â­ | 500-700ms | 0.5-0.6 | Balanced, good for most users (current) |
| **Careful Listening** | 800-1200ms | 0.6-0.7 | Patient, allows longer pauses |
| **Noisy Environment** | 600-800ms | 0.7-0.8 | Less sensitive, reduces false triggers |

### How to Change Settings

1. Edit [app/services/openai_service.py:307-312](../../caresma-backend/app/services/openai_service.py#L307-L312)
2. Adjust `silence_duration_ms` (recommended: 300-1500ms)
3. Adjust `threshold` (0.0 = very sensitive, 1.0 = less sensitive)
4. Restart backend server
5. Test with real users

### Troubleshooting

**Problem: System responds too quickly (cuts off user)**
- âœ… Increase `silence_duration_ms` to 700-1000ms
- âœ… Decrease `threshold` to 0.4-0.5 (less sensitive)

**Problem: System takes too long to respond**
- âœ… Decrease `silence_duration_ms` to 300-400ms
- âœ… Increase `threshold` to 0.6-0.7 (more sensitive)

**Problem: False triggers in noisy environment**
- âœ… Increase `threshold` to 0.7-0.8
- âœ… Keep `silence_duration_ms` at 500-700ms

---

## Technology Stack

### Frontend
- **React** - UI framework
- **@heygen/streaming-avatar** - Avatar video/TTS
- **WebSocket** - Real-time audio streaming
- **Web Audio API** - Microphone capture

### Backend
- **FastAPI** - Python web framework
- **WebSocket** - Real-time communication
- **OpenAI Realtime API** - ASR + LLM + VAD

### External Services
- **OpenAI** - Speech recognition, language model, voice activity detection
- **HeyGen** - Avatar rendering, lip-sync, video streaming

